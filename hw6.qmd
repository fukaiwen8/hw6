---
title: "Homework 6"
author: "[Kaiwen Fu]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
execute: 
  freeze: true
  cache: true
# format:
  html: # comment this line to get pdf
  pdf: 
    fig-width: 7
    fig-height: 7
---


::: {.callout-important style="font-size: 0.8em;"}

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::


In this assignment, we will perform various tasks involving principal component analysis (PCA), principal component regression, and dimensionality reduction.

We will need the following packages:


```{R, message=FALSE, warning=FALSE, results='hide'}
packages <- c(
  "tibble",
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "broom",
  "magrittr",
  "corrplot",
  "car"
)
# renv::install(packages)
sapply(packages, require, character.only=T)
```

<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 70 points
Principal component anlaysis and variable selection
:::

###### 1.1 (5 points)


The `data` folder contains a `spending.csv` dataset which is an illustrative sample of monthly spending data for a group of $5000$ people across a variety of categories. The response variable, `income`, is their monthly income, and objective is to predict the `income` for a an individual based on their spending patterns.

Read the data file as a tibble in R. Preprocess the data such that:

1. the variables are of the right data type, e.g., categorical variables are encoded as factors
2. all column names to lower case for consistency
3. Any observations with missing values are dropped

```R
path <- "data/spending.csv"

df <- ... # Insert your code here
```
```{R}
library(tibble)
library(dplyr)
library(readr)

path <- "data/spending.csv"
df <- read_csv(path) %>%
  mutate_if(is.character, as.factor) %>%
  rename_all(tolower) %>% 
  drop_na()  

```

---

###### 1.2 (5 points)

Visualize the correlation between the variables using the `corrplot()` function. What do you observe? What does this mean for the model?

```R
df_x  %>% ... # Insert your code here
```
```{R}
library(corrplot)
correlation_matrix <- cor(df %>% select(-one_of("income")))

corrplot(correlation_matrix, method = "circle")

```

---

###### 1.3 (5 points)

Run a linear regression model to predict the `income` variable using the remaining predictors. Interpret the coefficients and summarize your results. 


```R
... # Insert your code here
```
```{R}
library(car)

model <- lm(income ~ ., data = df)
summary(model) 

```

---

###### 1.3 (5 points)

Diagnose the model using the `vif()` function. What do you observe? What does this mean for the model?

```R
... # Insert your code here
```
```{R}
vif_values <- vif(model) 
print(vif_values)

```

---

###### 1.4 (5 points)

Perform PCA using the `princomp` function in R. Print the summary of the PCA object.

```R
pca <- ... # Insert your code here
... # Insert your code here
```
```{R}
pca <- prcomp(df[, -which(names(df) == "income")], scale. = TRUE)  
summary(pca) 

```

---

###### 1.5 (5 points)

Make a screeplot of the proportion of variance explained by each principal component. How many principal components would you choose to keep? Why?

```R
... # Insert your code here
```
```{R}
screeplot(pca, type = "lines")

```


###### 1.6 (5 points)

By setting any factor loadings below $0.2$ to $0$, summarize the factor loadings for the principal components that you chose to keep. 

```R
clean_loadings <- ... # Insert your code here
```
```{R}
loadings <- pca$rotation 

clean_loadings <- apply(loadings, 2, function(x) ifelse(abs(x) < 0.2, 0, x))

clean_loadings <- as.data.frame(clean_loadings)

summary(clean_loadings)



```


Visualize the factor loadings. 

```R
... # Insert your code here
```
```{R}
library(ggplot2)
library(tidyr)

long_loadings <- pivot_longer(clean_loadings, cols = everything(), names_to = 'Variable', values_to = 'Loading')

ggplot(long_loadings, aes(x = Variable, y = Loading, fill = Variable)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  coord_flip() + 
  labs(title = "Factor Loadings Visualization", x = "Principal Component", y = "Loading")


```

---

###### 1.7 (15 points)

Based on the factor loadings, what do you think the principal components represent? 

Provide an interpreation for each principal component you chose to keep.
```{R}
#Principal component analysis is like finding the best angle to take a collective photo of a group of items so that you can see the most obvious differences at a glance. Each angle, or principal component, reveals a different characteristic between the items: the first angle shows the most fundamental difference, the second angle the second most important difference, and so on.
```

---

###### 1.8 (10 points)

Create a new data frame with the original response variable `income` and the principal components you chose to keep. Call this data frame `df_pca`.

```R
... # Insert your code here
```
```{R}
num_components <- 3 

df_pca <- data.frame(income = df$income, pca$x[, 1:num_components])



```

Fit a regression model to predict the `income` variable using the principal components you chose to keep. Interpret the coefficients and summarize your results. 

```R
... # Insert your code here
```
```{R}
model_pca <- lm(income ~ ., data = df_pca)


summary(model_pca)
```


Compare the results of the regression model in 1.3 and 1.9. What do you observe? What does this mean for the model?
```{R}
summary(model) 
summary(model_pca)  

```

---

###### 1.10 (10 points)

Based on your interpretation of the principal components from Question 1.7, provide an interpretation of the regression model in Question 1.9.

```{R}
#Comparing the results of the two models, I found that the model simplified through principal component analysis predicts revenue very well, and this approach also reduces the number of variables required. While such a model may not be easy to understand intuitively because it is built based on abstract features of the data, it provides us with a more refined and potentially more stable method of prediction.
```


---


:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---



::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::